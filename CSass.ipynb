{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanStoutGent/MSMP-B/blob/main/CSass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9NdSbqtbuDw"
      },
      "source": [
        "## Load data and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTG-0rWhWkCZ",
        "outputId": "1ec09cd3-1f0f-4166-8520-6b72ac8e04fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.10/dist-packages (4.6.3)\n",
            "Requirement already satisfied: strsimpy in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install textdistance\n",
        "!pip install strsimpy\n",
        "import textdistance\n",
        "from strsimpy.qgram import QGram\n",
        "from strsimpy.normalized_levenshtein import NormalizedLevenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-1bjt8Q8akb",
        "outputId": "7ceb1e17-ff2e-4a05-966b-05b0ad98bc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sympy\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import nltk\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from itertools import combinations\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEo3sJ3pVBBR"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "X7BFY49BcfPg"
      },
      "outputs": [],
      "source": [
        "def remove_spaces_tokens(text):\n",
        "    return re.sub(r'^[\\s\\W]+', '', text.group(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "7hs2tYNLVGke"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\((.*?)\\)', r'\\1', text)\n",
        "  placeholder = \"UNIQUE_PLACEHOLDER\"\n",
        "\n",
        "  to_inch = [r'\\bInch\\b', r'inches', r'[\"”]', r'-inch', r' inch', r'inch']\n",
        "  for variant in to_inch:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'inch')\n",
        "\n",
        "  to_hz = [r'Hertz', r'hertz', r'Hz', r'HZ', r' hz', r'-hz', r'hz']\n",
        "  for variant in to_hz:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'hz')\n",
        "\n",
        "  to_lbs = [r'lbs\\.',r'lb\\.', r'lbs', r'lb', r'pounds', r'pound']\n",
        "  for variant in to_lbs:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'lbs')\n",
        "\n",
        "  to_nit = [r'nit', r' cd/mÂ²', r'cd/mÂ²', r' cd/m²', r'cd/m²', r' cd/m2', r'cd/m2', r'Nit' r'cd/mÂ²', r'cd/mâ²']\n",
        "  for variant in to_nit:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'nit')\n",
        "\n",
        "  to_hrs = [r'hours', r'hour', r'hrs', r'Hrs (typical)', r'hrs (typical)']\n",
        "  for variant in to_hrs:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'hrs')\n",
        "\n",
        "  to_w = [r'watts', r'watt', r'w']\n",
        "  for variant in to_w:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'w')\n",
        "\n",
        "  to_mw = [r'mw', r' mw', r'Mw', r'MW', r' Mw']\n",
        "  for variant in to_mw:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'mw')\n",
        "\n",
        "  text = re.sub(r'[\\s\\W]*(inch|hz|wifi|mw|lbs|hrs|w|nit)\\b', remove_spaces_tokens, text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "WXTLB74AdU9f"
      },
      "outputs": [],
      "source": [
        "def clean_features_title(input_file,output_file):\n",
        "  with open(input_file, 'r') as file:\n",
        "    tv_data = copy.deepcopy(json.load(file))\n",
        "\n",
        "  for model_id, tvs in tv_data.items():\n",
        "    for tv in tvs:\n",
        "      if 'title' in tv:\n",
        "        tv['title'] = clean_text(tv['title'])\n",
        "      if 'featuresMap' in tv:\n",
        "        for feature, value in tv['featuresMap'].items():\n",
        "          cleaned_feature = clean_text(value)\n",
        "          tv['featuresMap'][feature] = cleaned_feature\n",
        "\n",
        "  with open(output_file, 'w') as file:\n",
        "      json.dump(tv_data, file, indent=4)\n",
        "\n",
        "  return output_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaxX0-qfHsGY"
      },
      "source": [
        "## Binary Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "qQcE8gClRJLO"
      },
      "outputs": [],
      "source": [
        "def get_brand_dictionary(tvs):\n",
        "\n",
        "  all_brand_names = set()\n",
        "  for tv in tvs:\n",
        "    features_map = tv.get('featuresMap', {})\n",
        "    brand = features_map.get('Brand') or features_map.get('Brand Name') or features_map.get('Brand Name:')\n",
        "    if brand:\n",
        "      all_brand_names.add(brand)\n",
        "\n",
        "  return list(all_brand_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "4y6VkBWvK8Rl"
      },
      "outputs": [],
      "source": [
        "def get_mw_title(title):\n",
        "\n",
        "  mw_pattern = r'([a-zA-Z0-9]*(([0-9]+[^0-9, ]+)|([^0-9, ]+[0-9]+))[a-zA-Z0-9]*)'\n",
        "  regex = re.compile(mw_pattern)\n",
        "  model_words = [match[0] for match in regex.findall(title)]\n",
        "\n",
        "  return model_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "0EpWUcVBstqR"
      },
      "outputs": [],
      "source": [
        "def get_title_dictionary(titles_list):\n",
        "  titles_dictionary = set()\n",
        "\n",
        "  for title in titles_list:\n",
        "    mw_title = get_mw_title(title)\n",
        "    titles_dictionary.update(mw_title)\n",
        "\n",
        "  return list(titles_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "LDufCVWwUKWF"
      },
      "outputs": [],
      "source": [
        "def get_brand_tv(tv, brand_names):\n",
        "\n",
        "  title  = tv['title']\n",
        "  features_map = tv.get('featuresMap', {})\n",
        "  brand = features_map.get('Brand') or features_map.get('Brand Name') or features_map.get('Brand Name:')\n",
        "\n",
        "  if brand:\n",
        "    brand_name = brand\n",
        "  else:\n",
        "    for name in brand_names:\n",
        "      if name in title.lower():\n",
        "        brand_name = name\n",
        "        break\n",
        "      else:\n",
        "        brand_name = 'unknown'\n",
        "\n",
        "  return brand_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "AS0zWC1pLAep"
      },
      "outputs": [],
      "source": [
        "def get_mw_feature(feature):\n",
        "  #unit_list = ['inch','hz','wifi','mw','lbs','nit','w','hrs']\n",
        "  unit_list = []\n",
        "  feature_pattern = r'(^\\d+(\\.\\d+)?[a-zA-Z]*$|^\\d+(\\.\\d+)?$)'\n",
        "  regex = re.compile(feature_pattern)\n",
        "  matches = regex.findall(feature)\n",
        "\n",
        "  model_words = []\n",
        "  tokens = feature.split()\n",
        "  for token in tokens:\n",
        "    matches = regex.findall(token)\n",
        "    for match in matches:\n",
        "      full_match = match[0]\n",
        "\n",
        "\n",
        "      if any(full_match.endswith(unit) for unit in unit_list):\n",
        "        model_words.append(full_match)\n",
        "\n",
        "      elif re.match(r'^\\d+(\\.\\d+)?', full_match):\n",
        "        numeric_part = re.match(r'^\\d+(\\.\\d+)?', full_match).group()\n",
        "        model_words.append(numeric_part)\n",
        "\n",
        "  return list(model_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "XHTUkooNtUXq"
      },
      "outputs": [],
      "source": [
        "def get_feature_dictionary(features_maps):\n",
        "  features_dictionary = set()\n",
        "\n",
        "  for feature_map in features_maps:\n",
        "    for feature, value in feature_map.items():\n",
        "      mw_feature = get_mw_feature(value)\n",
        "      features_dictionary.update(mw_feature)\n",
        "\n",
        "  return list(features_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "ZajDakryHVJq"
      },
      "outputs": [],
      "source": [
        "def get_tv_dictionary(tv,brand_dictionary):\n",
        "\n",
        "  title = tv['title']\n",
        "  feature_map = tv.get('featuresMap', {})\n",
        "  brand = set(get_brand_tv(tv, brand_dictionary))\n",
        "\n",
        "  tv_dictionary_title = set(get_mw_title(title))\n",
        "\n",
        "  tv_dictionary_feature = set()\n",
        "  for feature, value in feature_map.items():\n",
        "    mw_feature = get_mw_feature(value)\n",
        "    tv_dictionary_feature.update(mw_feature)\n",
        "\n",
        "  tv_dictionary = set(tv_dictionary_title.union(tv_dictionary_feature).union(brand))\n",
        "\n",
        "  return list(tv_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "UulCXokmH1Wu"
      },
      "outputs": [],
      "source": [
        "def get_binary_matrix(all_tvs):\n",
        "\n",
        "  tv_titles = [tv['title'] for tv in all_tvs]\n",
        "  brand_dictionary = set(get_brand_dictionary(all_tvs))\n",
        "  title_dictionary = set(get_title_dictionary(tv_titles))\n",
        "  feature_maps = [tv.get('featuresMap', {}) for tv in all_tvs]\n",
        "  feature_dictionary = set(get_feature_dictionary(feature_maps))\n",
        "  dictionary = set(brand_dictionary.union(title_dictionary).union(feature_dictionary))\n",
        "\n",
        "  binary_matrix = np.zeros((len(dictionary), len(all_tvs)), dtype=int)\n",
        "\n",
        "  for tv_index, tv in enumerate(all_tvs):\n",
        "    feature_map = tv.get('featuresMap', {})\n",
        "    title = tv['title']\n",
        "    brand = get_brand_tv(tv, brand_dictionary)\n",
        "    tv_dictionary = set(get_tv_dictionary(tv, brand_dictionary))\n",
        "    for word_index, word in enumerate(dictionary):\n",
        "      if word in tv_dictionary:\n",
        "        binary_matrix[word_index, tv_index] = 1\n",
        "\n",
        "  return binary_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVol4KAEH1me"
      },
      "source": [
        "## Minhashing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "wTdz1fHueBLE"
      },
      "outputs": [],
      "source": [
        "def hash_function(a, b, p, x):\n",
        "  return (a * x + b) % p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "XlHAUNjzH-FD"
      },
      "outputs": [],
      "source": [
        "def get_hashes(n_rows, n_hashes):\n",
        "  n_hashes = n_hashes\n",
        "  p = sympy.nextprime(n_rows)\n",
        "\n",
        "  hash_functions = []\n",
        "  for i in range(n_hashes):\n",
        "    a = random.randint(1, p - 1)\n",
        "    b = random.randint(0, p - 1)\n",
        "    hash_functions.append((a, b, p))\n",
        "\n",
        "  return hash_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "tFFKWI4SkST-"
      },
      "outputs": [],
      "source": [
        "def minhashing(binary_matrix, n_hashes):\n",
        "  n_rows, n_cols = binary_matrix.shape\n",
        "  hash_functions = get_hashes(n_rows, n_hashes)\n",
        "  n_hashes = len(hash_functions)\n",
        "\n",
        "  signature_matrix = np.full((n_hashes, n_cols), float('inf'))\n",
        "\n",
        "  for j in range(n_cols):\n",
        "    for i in range(n_rows):\n",
        "      if binary_matrix[i,j] == 1:\n",
        "        for k in range(n_hashes):\n",
        "          a, b, p = hash_functions[k]\n",
        "          hash_value = hash_function(a, b, p, i)\n",
        "          signature_matrix[k, j] = min(signature_matrix[k, j], hash_value)\n",
        "\n",
        "  return signature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgOLDOrWH-YP"
      },
      "source": [
        "## LSH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "-S6A4_72H_21"
      },
      "outputs": [],
      "source": [
        "def lsh(signature_matrix, b,r):\n",
        "  n_hashes, n_cols = signature_matrix.shape\n",
        "\n",
        "  if int(n_hashes) != b * r:\n",
        "    raise ValueError(\"The number of rows in signature matrix must be equal to b * r.\")\n",
        "\n",
        "  buckets = {}\n",
        "  candidate_pairs = set()\n",
        "\n",
        "  for i in range(b):\n",
        "    for j in range(n_cols):\n",
        "      start_band = i * r\n",
        "      end_band = (i + 1) * r\n",
        "      signature_band = tuple(signature_matrix[start_band:end_band, j])\n",
        "      buckets.setdefault(signature_band, []).append(j)\n",
        "\n",
        "  for bucket in buckets.values():\n",
        "    bucket = list(set(bucket))\n",
        "    if len(bucket) > 1:\n",
        "      for i in range(len(bucket)):\n",
        "        for j in range(i + 1, len(bucket)):\n",
        "          pair = tuple(sorted((bucket[i], bucket[j])))\n",
        "          candidate_pairs.add(pair)\n",
        "\n",
        "  return list(candidate_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjE4EEaFJFFZ"
      },
      "source": [
        "## Baseline code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "1L624kY6JESn"
      },
      "outputs": [],
      "source": [
        "def clean_text_old(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\((.*?)\\)', r'\\1', text)\n",
        "  placeholder = \"UNIQUE_PLACEHOLDER\"\n",
        "\n",
        "  to_inch = [r'Inch', r'inches', r'[\"”]', r'-inch', r' inch', r'inch']\n",
        "  for variant in to_inch:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'inch')\n",
        "\n",
        "  to_hz = [r'Hertz', r'hertz', r'Hz', r'HZ', r' hz', r'-hz', r'hz']\n",
        "  for variant in to_hz:\n",
        "    text = re.sub(variant, placeholder, text)\n",
        "  text = text.replace(placeholder, 'hz')\n",
        "\n",
        "  text = re.sub(r'[\\s\\W]*(inch|hz)\\b', remove_spaces_tokens, text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "2iyixtsRJiOM"
      },
      "outputs": [],
      "source": [
        "def clean_features_title_old(input_file,output_file):\n",
        "  with open(input_file, 'r') as file:\n",
        "    tv_data = copy.deepcopy(json.load(file))\n",
        "\n",
        "  for model_id, tvs in tv_data.items():\n",
        "    for tv in tvs:\n",
        "      if 'title' in tv:\n",
        "        tv['title'] = clean_text_old(tv['title'])\n",
        "      if 'featuresMap' in tv:\n",
        "        for feature, value in tv['featuresMap'].items():\n",
        "          cleaned_feature = clean_text_old(value)\n",
        "          tv['featuresMap'][feature] = cleaned_feature\n",
        "\n",
        "  with open(output_file, 'w') as file:\n",
        "      json.dump(tv_data, file, indent=4)\n",
        "\n",
        "  return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "MOO4I5sIKWZI"
      },
      "outputs": [],
      "source": [
        "def get_tv_dictionary_old(tv):\n",
        "\n",
        "  title = tv['title']\n",
        "  feature_map = tv.get('featuresMap', {})\n",
        "  tv_dictionary_title = set(get_mw_title(title))\n",
        "\n",
        "  tv_dictionary_feature = set()\n",
        "  for feature, value in feature_map.items():\n",
        "    mw_feature = get_mw_feature(value)\n",
        "    tv_dictionary_feature.update(mw_feature)\n",
        "\n",
        "  tv_dictionary = set(tv_dictionary_title.union(tv_dictionary_feature))\n",
        "\n",
        "  return list(tv_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "vTSnxe-VKfWO"
      },
      "outputs": [],
      "source": [
        "def get_binary_matrix_old(all_tvs):\n",
        "\n",
        "  tv_titles = [tv['title'] for tv in all_tvs]\n",
        "  title_dictionary = set(get_title_dictionary(tv_titles))\n",
        "  feature_maps = [tv.get('featuresMap', {}) for tv in all_tvs]\n",
        "  feature_dictionary = set(get_feature_dictionary(feature_maps))\n",
        "  dictionary = set(title_dictionary.union(feature_dictionary))\n",
        "\n",
        "  binary_matrix = np.zeros((len(dictionary), len(all_tvs)), dtype=int)\n",
        "\n",
        "  for tv_index, tv in enumerate(all_tvs):\n",
        "    feature_map = tv.get('featuresMap', {})\n",
        "    title = tv['title']\n",
        "    tv_dictionary = set(get_tv_dictionary_old(tv))\n",
        "    for word_index, word in enumerate(dictionary):\n",
        "      if word in tv_dictionary:\n",
        "        binary_matrix[word_index, tv_index] = 1\n",
        "\n",
        "  return binary_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKJR_R-_H_9a"
      },
      "source": [
        "## MSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "q3jGKyxg9V9G"
      },
      "outputs": [],
      "source": [
        "def get_qGramSimilarity(string1, string2, q):\n",
        "\n",
        "  qgram = QGram(q)\n",
        "  distance = (len(string1) + len(string2) - qgram.distance(string1, string2)) / (len(string1) + len(string2))\n",
        "\n",
        "  return(distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "qEQ6nC_vKC_9"
      },
      "outputs": [],
      "source": [
        "def get_feature_similarity(tv1, tv2, q, gamma):\n",
        "\n",
        "  feature_map1 = tv1.get('featuresMap', {})\n",
        "  keys1 = set(feature_map1.keys())\n",
        "  feature_map2 = tv2.get('featuresMap', {})\n",
        "  keys2 = set(feature_map2.keys())\n",
        "\n",
        "  total_similarity = 0\n",
        "  weighted_key_similarity = 0\n",
        "  num_matches = 0\n",
        "\n",
        "  unmatched_keys_1 = set(keys1)\n",
        "  unmatched_keys_2 = set(keys2)\n",
        "\n",
        "  for key_1, value_1 in feature_map1.items():\n",
        "    for key_2, value_2 in feature_map2.items():\n",
        "      key_similarity = get_qGramSimilarity(key_1, key_2, q)\n",
        "\n",
        "      if key_similarity >= gamma:\n",
        "        value_similarity = get_qGramSimilarity(value_1, value_2, q)\n",
        "        total_similarity += key_similarity * value_similarity\n",
        "        weighted_key_similarity += key_similarity\n",
        "        num_matches += 1\n",
        "\n",
        "        if key_1 in unmatched_keys_1:\n",
        "          unmatched_keys_1.remove(key_1)\n",
        "        if key_2 in unmatched_keys_2:\n",
        "          unmatched_keys_2.remove(key_2)\n",
        "\n",
        "  matching_similarity = 0\n",
        "  if weighted_key_similarity > 0:\n",
        "    matching_similarity = total_similarity / weighted_key_similarity\n",
        "\n",
        "  unmactched_values1 = [feature_map1[key] for key in unmatched_keys_1]\n",
        "  unmactched_values2 = [feature_map2[key] for key in unmatched_keys_2]\n",
        "\n",
        "  model_words1 = set()\n",
        "  for value in unmactched_values1:\n",
        "    model_words1.update(get_mw_feature(value))\n",
        "  model_words2 = set()\n",
        "  for value in unmactched_values2:\n",
        "    model_words2.update(get_mw_feature(value))\n",
        "\n",
        "  non_matching_similarity = 0\n",
        "  if model_words1 or model_words2:\n",
        "    non_matching_similarity = len(model_words1.intersection(model_words2)) / len(model_words1.union(model_words2))\n",
        "\n",
        "  return matching_similarity, non_matching_similarity, num_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "zjiHwoeCgofl"
      },
      "outputs": [],
      "source": [
        "def get_average_LS_similarity(X,Y):\n",
        "\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "\n",
        "  for x in X:\n",
        "    for y in Y:\n",
        "      numerator += (1 - textdistance.levenshtein.normalized_distance(x, y)) * (len(x) + len(y))\n",
        "      denominator += len(x) + len(y)\n",
        "\n",
        "  return(numerator / denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "KQilehnOm0NU"
      },
      "outputs": [],
      "source": [
        "def split_mw(model_word):\n",
        "\n",
        "    non_numeric = re.sub(r'[0-9]', '', model_word)\n",
        "    numeric = re.sub(r'[^0-9]', '', model_word)\n",
        "\n",
        "    return non_numeric, numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "23Rk5ntoYlV9"
      },
      "outputs": [],
      "source": [
        "def get_title_similarity(title1, title2, brand_dictionary, alpha, beta, delta, epsilon):\n",
        "\n",
        "  noisy_words = set(['best', 'buy', 'newegg.com', 'thenerdsnet', 'refurbished', 'open', 'box'])\n",
        "  words_to_remove = set(stopwords.words('english')).union(noisy_words)\n",
        "\n",
        "  clean_title1 = re.sub(r'[^a-zA-Z0-9. ]', ' ', title1)\n",
        "  clean_title2 = re.sub(r'[^a-zA-Z0-9. ]', ' ', title2)\n",
        "  clean_title1 = set([word for word in clean_title1.split(' ') if word not in words_to_remove])\n",
        "  clean_title2 = set([word for word in clean_title2.split(' ') if word not in words_to_remove])\n",
        "\n",
        "  titleCosine = len(clean_title1.intersection(clean_title2)) / (math.sqrt(len(clean_title1)) * math.sqrt(len(clean_title2)))\n",
        "\n",
        "  if titleCosine >= alpha:\n",
        "    return 1\n",
        "\n",
        "  #Compute weighted sim\n",
        "  mw_title1 = get_mw_title(title1)\n",
        "  mw_title2 = get_mw_title(title2)\n",
        "\n",
        "  for mw_i in mw_title1:\n",
        "    for mw_j in mw_title2:\n",
        "      non_numeric_i, numeric_i = split_mw(mw_i)\n",
        "      non_numeric_j, numeric_j = split_mw(mw_j)\n",
        "      if textdistance.levenshtein.normalized_distance(non_numeric_i, non_numeric_j) < epsilon and numeric_i != numeric_j:\n",
        "        return -1\n",
        "\n",
        "  similarity_title = beta * titleCosine + (1 - beta) * get_average_LS_similarity(clean_title1, clean_title2)\n",
        "\n",
        "  similar = False\n",
        "  num = 0\n",
        "  denom = 0\n",
        "  for mw_i in mw_title1:\n",
        "    for mw_j in mw_title2:\n",
        "      non_numeric_i, numeric_i = split_mw(mw_i)\n",
        "      non_numeric_j, numeric_j = split_mw(mw_j)\n",
        "\n",
        "      if numeric_i == numeric_j and textdistance.levenshtein.normalized_distance(non_numeric_i, non_numeric_j) < epsilon:\n",
        "        similar = True\n",
        "        num += (1 - textdistance.levenshtein.normalized_distance(mw_i, mw_j)) * len(mw_i) * len(mw_j)\n",
        "        denom += len(mw_i) + len(mw_j)\n",
        "\n",
        "  if similar:\n",
        "    ls_similarity_titles = num/denom\n",
        "    similarity_title = delta * ls_similarity_titles + (1 - delta) * similarity_title\n",
        "\n",
        "  return similarity_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "5zUDtanEmiLD"
      },
      "outputs": [],
      "source": [
        "def get_dissimilarity_matrix(tvs, candidate_pairs, q, alpha, beta, gamma, delta, mu, epsilon):\n",
        "\n",
        "  matrix = np.ones((len(tvs), len(tvs))) * 10000000000\n",
        "  brand_dictionary = get_brand_dictionary(tvs)\n",
        "\n",
        "  for pair in candidate_pairs:\n",
        "    tv_i = tvs[pair[0]]\n",
        "    tv_j = tvs[pair[1]]\n",
        "    brand_i = get_brand_tv(tv_i, brand_dictionary)\n",
        "    brand_j = get_brand_tv(tv_j, brand_dictionary)\n",
        "\n",
        "    if tv_i['shop'] != tv_j['shop'] and brand_i == brand_j:\n",
        "      matchingkey_similarity, non_matchingkey_similarity, num_matches = get_feature_similarity(tv_i, tv_j, q, gamma)\n",
        "      title_similarity = get_title_similarity(tv_i['title'], tv_j['title'], brand_dictionary, alpha, beta, delta, epsilon)\n",
        "\n",
        "      min_num_features = min(len(tv_i.get('featuresMap', {})), len(tv_j.get('featuresMap', {})))\n",
        "      if title_similarity == -1:\n",
        "        theta1 = num_matches / min_num_features if min_num_features > 0 else 0\n",
        "        theta2 = 1 - theta1\n",
        "        total_similarity = theta1 * matchingkey_similarity + theta2 * non_matchingkey_similarity\n",
        "      else:\n",
        "        theta1 = (1-mu) * (num_matches) / min_num_features if min_num_features > 0 else 0\n",
        "        theta2 = 1 - theta1 - mu\n",
        "        total_similarity = theta1 * matchingkey_similarity + theta2 * non_matchingkey_similarity + mu * title_similarity\n",
        "\n",
        "      total_dissimilarity = 1 - total_similarity\n",
        "      matrix[pair[0], pair[1]] = total_dissimilarity\n",
        "      matrix[pair[1], pair[0]] = total_dissimilarity\n",
        "\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "G6LK-3YW_qb2"
      },
      "outputs": [],
      "source": [
        "def hClustering(matrix, epsilon):\n",
        "\n",
        "  cluster = AgglomerativeClustering(\n",
        "      n_clusters=None,\n",
        "      linkage='single',\n",
        "      distance_threshold=epsilon,\n",
        "      metric='precomputed'\n",
        "  )\n",
        "  cluster_labels = cluster.fit_predict(matrix)\n",
        "\n",
        "  clusters = {}\n",
        "  for tv, cluster in enumerate(cluster_labels):\n",
        "    clusters.setdefault(cluster, []).append(tv)\n",
        "\n",
        "  duplicate_pairs = set()\n",
        "  for tvs in clusters.values():\n",
        "    if len(tvs) > 1:\n",
        "      for pair in combinations(tvs, 2):\n",
        "        duplicate_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "  return list(duplicate_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHOpOY18LGrF"
      },
      "source": [
        "##Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "WsTFqm6mLGHT"
      },
      "outputs": [],
      "source": [
        "def get_true_duplicates(all_tvs):\n",
        "\n",
        "  true_duplicates = set()\n",
        "\n",
        "  for i in range(len(all_tvs)):\n",
        "    for j in range(i + 1, len(all_tvs)):\n",
        "      if all_tvs[i]['modelID'] == all_tvs[j]['modelID']:\n",
        "        true_duplicates.add(tuple(sorted((i, j))))\n",
        "\n",
        "  return true_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "H-xvfLtBmM3C"
      },
      "outputs": [],
      "source": [
        "def get_pq_pc_f1star(true_pairs, candidate_pairs):\n",
        "\n",
        "  df = len(set(candidate_pairs).intersection(set(true_pairs)))\n",
        "  nc = len(set(candidate_pairs))\n",
        "  dn = len(set(true_pairs))\n",
        "\n",
        "  pq = df / nc if nc > 0 else 0\n",
        "  pc = df / dn if dn > 0 else 0\n",
        "\n",
        "  f1star = 0\n",
        "  if pc + pq > 0:\n",
        "    f1star = 2 * pc * pq / (pc + pq)\n",
        "\n",
        "  return pq, pc, f1star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "wmIpcJy-sdLg"
      },
      "outputs": [],
      "source": [
        "def get_pr_re_F1(true_pairs, predicted_pairs,candidate_pairs):\n",
        "  possible_trues = set(true_pairs).intersection(set(candidate_pairs))\n",
        "\n",
        "  tp = len(set(possible_trues).intersection(set(predicted_pairs)))\n",
        "  fn = len(set(possible_trues)) - tp\n",
        "  fp = len(set(predicted_pairs)) - tp\n",
        "\n",
        "  precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "  recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "  f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "  return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "bf9jbWaeLUR2"
      },
      "outputs": [],
      "source": [
        "def get_pr_re_F1_total(true_pairs, predicted_pairs,candidate_pairs):\n",
        "\n",
        "  tp = len(set(true_pairs).intersection(set(predicted_pairs)))\n",
        "  fn = len(set(true_pairs)) - tp\n",
        "  fp = len(set(predicted_pairs)) - tp\n",
        "\n",
        "  precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "  recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "  f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "  return precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEzg4cOlQNIv"
      },
      "source": [
        "##Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "AtFmJdB4f9a8"
      },
      "outputs": [],
      "source": [
        "def bootstrapLSH(all_tvs, n_hashes, iterations, old):\n",
        "  random.seed(1)\n",
        "\n",
        "  n_train = 1024\n",
        "  n_test = 600\n",
        "  n_products = len(all_tvs)\n",
        "\n",
        "  def getIterationResults(tvs,sigmatrix,n_hashes):\n",
        "    results_iter = []\n",
        "    for b in range(5, int(n_hashes//2 + 1)):\n",
        "      if n_hashes % b != 0:\n",
        "        continue\n",
        "      r = n_hashes // b\n",
        "      candidate_pairs = lsh(sigmatrix, b, r)\n",
        "      true_pairs = get_true_duplicates(tvs)\n",
        "      comparison_fraction = len(candidate_pairs) / total_comparisons\n",
        "      pq, pc, f1star = get_pq_pc_f1star(true_pairs, candidate_pairs)\n",
        "      results_iter.append([b, r, comparison_fraction, pq, pc, f1star])\n",
        "\n",
        "    return results_iter\n",
        "\n",
        "  results_total = []\n",
        "  for i in range(iterations):\n",
        "    print('iteration: ', i+1)\n",
        "    train_indices = sorted(np.random.choice(n_products, size=n_train, replace=True))\n",
        "    train_tvs = [all_tvs[i] for i in train_indices]\n",
        "    n_tvs = len(train_tvs)\n",
        "    total_comparisons = n_tvs * (n_tvs - 1) // 2\n",
        "\n",
        "    if old:\n",
        "      binary_matrix_train = get_binary_matrix_old(train_tvs)\n",
        "    else:\n",
        "      binary_matrix_train = get_binary_matrix(train_tvs)\n",
        "\n",
        "    sigmatrix_train = minhashing(binary_matrix_train, n_hashes)\n",
        "    iteration_results = getIterationResults(train_tvs,sigmatrix_train, n_hashes)\n",
        "    results_total.extend(iteration_results)\n",
        "\n",
        "  results_total = np.array(results_total)\n",
        "\n",
        "  results_dict = defaultdict(list)\n",
        "  for row in results_total:\n",
        "    b, r, comparison_fraction, pq, pc, f1star = row\n",
        "    results_dict[(int(b), int(r))].append([comparison_fraction, pq, pc, f1star])\n",
        "\n",
        "  averaged_results = []\n",
        "  for (b, r), metrics in results_dict.items():\n",
        "    metrics_array = np.array(metrics)\n",
        "    avg_metrics = metrics_array.mean(axis=0)\n",
        "    averaged_results.append([b, r] + avg_metrics.tolist())\n",
        "\n",
        "  averaged_results = np.array(averaged_results)\n",
        "\n",
        "  return averaged_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "Aby3AFqVSTaC"
      },
      "outputs": [],
      "source": [
        "def bootstrapMSM(all_tvs, n_hashes, n_bootsraps, old):\n",
        "  random.seed(1)\n",
        "\n",
        "  alpha = 0.602\n",
        "  beta = 0.00\n",
        "  gamma = 0.756\n",
        "  delta = 0.7\n",
        "  mu = 0.65\n",
        "  epsilon = 0.522\n",
        "  distance_tresholds = [i/20 for i in range(1,20)]\n",
        "  q = 3\n",
        "\n",
        "  n_train = 1024\n",
        "  n_test = 600\n",
        "  n_products = len(all_tvs)\n",
        "\n",
        "  results_total = []\n",
        "  for i in range(n_bootsraps):\n",
        "    print('iteration: ', i+1)\n",
        "    train_indices = sorted(np.random.choice(n_products, size=n_train, replace=True))\n",
        "    all_indices = set(range(n_products))\n",
        "    test_indices = sorted(list(all_indices - set(train_indices)))\n",
        "    train_tvs = [all_tvs[i] for i in train_indices]\n",
        "    test_tvs = [all_tvs[i] for i in test_indices]\n",
        "\n",
        "    if old:\n",
        "      train_binary = get_binary_matrix_old(train_tvs)\n",
        "      test_binary = get_binary_matrix_old(test_tvs)\n",
        "    else:\n",
        "      train_binary = get_binary_matrix(train_tvs)\n",
        "      test_binary = get_binary_matrix(test_tvs)\n",
        "    train_signature = minhashing(train_binary, n_hashes)\n",
        "\n",
        "    n_test_tvs = len(test_tvs)\n",
        "    total_comparisons_test = n_test_tvs * (n_test_tvs - 1) // 2\n",
        "    test_signature = minhashing(test_binary,n_hashes)\n",
        "\n",
        "    results_iter = []\n",
        "    for b in range(5, int(n_hashes//3 + 1)):\n",
        "      if n_hashes % b != 0:\n",
        "        continue\n",
        "      r = n_hashes // b\n",
        "      train_candidate_pairs = lsh(train_signature, int(b), int(r))\n",
        "      train_true_pairs = get_true_duplicates(train_tvs)\n",
        "      train_similarity_matrix = get_dissimilarity_matrix(train_tvs, train_candidate_pairs, q, alpha, beta, gamma, delta, mu, epsilon)\n",
        "\n",
        "      train_best_f1 = -np.inf\n",
        "      for treshold in distance_tresholds:\n",
        "        train_predicted_pairs = hClustering(train_similarity_matrix, treshold)\n",
        "        train_precision, train_recall, train_f1 = get_pr_re_F1(train_true_pairs, train_predicted_pairs, train_candidate_pairs)\n",
        "        a,c, train_f1_total = get_pr_re_F1_total(train_true_pairs, train_predicted_pairs, train_candidate_pairs)\n",
        "        if train_f1 > train_best_f1:\n",
        "          train_best_f1 = train_f1\n",
        "          best_treshold = treshold\n",
        "\n",
        "      test_candidate_pairs = lsh(test_signature, int(b), int(r))\n",
        "      test_similarity_matrix = get_dissimilarity_matrix(test_tvs, test_candidate_pairs, q, alpha, beta, gamma, delta, mu, epsilon)\n",
        "      test_predicted_pairs = hClustering(test_similarity_matrix, best_treshold)\n",
        "      test_true_pairs = get_true_duplicates(test_tvs)\n",
        "      test_precision, test_recall, test_f1 = get_pr_re_F1(test_true_pairs, test_predicted_pairs, test_candidate_pairs)\n",
        "      a,c, test_f1_total = get_pr_re_F1_total(test_true_pairs, test_predicted_pairs, test_candidate_pairs)\n",
        "\n",
        "      test_comparison_fraction = len(test_candidate_pairs) / total_comparisons_test\n",
        "      results_iter.append([b, r, test_comparison_fraction, test_precision, test_recall, test_f1,test_f1_total])\n",
        "\n",
        "    results_total.extend(results_iter)\n",
        "\n",
        "  results_total = np.array(results_total)\n",
        "  results_dict = defaultdict(list)\n",
        "\n",
        "  for row in results_total:\n",
        "    b, r, test_comparison_fraction, test_precision, test_recall,test_f1, test_f1_total = row\n",
        "    results_dict[(int(b), int(r))].append([test_comparison_fraction, test_precision, test_recall, test_f1,test_f1_total])\n",
        "\n",
        "  averaged_results = []\n",
        "  for (b, r), metrics in results_dict.items():\n",
        "    metrics_array = np.array(metrics)\n",
        "    avg_metrics = metrics_array.mean(axis=0)\n",
        "    averaged_results.append([b, r] + avg_metrics.tolist())\n",
        "\n",
        "  averaged_results = np.array(averaged_results)\n",
        "\n",
        "  return averaged_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrapMSMtotal(all_tvs, n_hashes, n_bootsraps, old):\n",
        "  random.seed(1)\n",
        "\n",
        "  alpha = 0.602\n",
        "  beta = 0.00\n",
        "  gamma = 0.756\n",
        "  delta = 0.7\n",
        "  mu = 0.65\n",
        "  epsilon = 0.522\n",
        "  distance_tresholds = [i/20 for i in range(1,20)]\n",
        "  q = 3\n",
        "\n",
        "  n_train = 1024\n",
        "  n_test = 600\n",
        "  n_products = len(all_tvs)\n",
        "\n",
        "  results_total = []\n",
        "  for i in range(n_bootsraps):\n",
        "    print('iteration: ', i+1)\n",
        "    train_indices = sorted(np.random.choice(n_products, size=n_train, replace=True))\n",
        "    all_indices = set(range(n_products))\n",
        "    test_indices = sorted(list(all_indices - set(train_indices)))\n",
        "    train_tvs = [all_tvs[i] for i in train_indices]\n",
        "    test_tvs = [all_tvs[i] for i in test_indices]\n",
        "\n",
        "    if old:\n",
        "      train_binary = get_binary_matrix_old(train_tvs)\n",
        "      test_binary = get_binary_matrix_old(test_tvs)\n",
        "    else:\n",
        "      train_binary = get_binary_matrix(train_tvs)\n",
        "      test_binary = get_binary_matrix(test_tvs)\n",
        "    train_signature = minhashing(train_binary, n_hashes)\n",
        "\n",
        "    n_test_tvs = len(test_tvs)\n",
        "    total_comparisons_test = n_test_tvs * (n_test_tvs - 1) // 2\n",
        "    test_signature = minhashing(test_binary,n_hashes)\n",
        "\n",
        "    results_iter = []\n",
        "    for b in range(5, int(n_hashes//4 + 1)):\n",
        "      if n_hashes % b != 0:\n",
        "        continue\n",
        "      r = n_hashes // b\n",
        "      train_candidate_pairs = lsh(train_signature, int(b), int(r))\n",
        "      train_true_pairs = get_true_duplicates(train_tvs)\n",
        "      train_similarity_matrix = get_dissimilarity_matrix(train_tvs, train_candidate_pairs, q, alpha, beta, gamma, delta, mu, epsilon)\n",
        "\n",
        "      train_best_f1 = -np.inf\n",
        "      for treshold in distance_tresholds:\n",
        "        train_predicted_pairs = hClustering(train_similarity_matrix, treshold)\n",
        "        train_precision, train_recall, train_f1 = get_pr_re_F1(train_true_pairs, train_predicted_pairs, train_candidate_pairs)\n",
        "        a,c, train_f1_total = get_pr_re_F1_total(train_true_pairs, train_predicted_pairs, train_candidate_pairs)\n",
        "        if train_f1_total > train_best_f1:\n",
        "          train_best_f1 = train_f1_total\n",
        "          best_treshold = treshold\n",
        "\n",
        "      test_candidate_pairs = lsh(test_signature, int(b), int(r))\n",
        "      test_similarity_matrix = get_dissimilarity_matrix(test_tvs, test_candidate_pairs, q, alpha, beta, gamma, delta, mu, epsilon)\n",
        "      test_predicted_pairs = hClustering(test_similarity_matrix, best_treshold)\n",
        "      test_true_pairs = get_true_duplicates(test_tvs)\n",
        "      test_precision, test_recall, test_f1 = get_pr_re_F1(test_true_pairs, test_predicted_pairs, test_candidate_pairs)\n",
        "      a,b, test_f1_total = get_pr_re_F1_total(test_true_pairs, test_predicted_pairs, test_candidate_pairs)\n",
        "\n",
        "      test_comparison_fraction = len(test_candidate_pairs) / total_comparisons_test\n",
        "      results_iter.append([b, r, test_comparison_fraction, test_precision, test_recall, test_f1,test_f1_total])\n",
        "\n",
        "    results_total.extend(results_iter)\n",
        "\n",
        "  results_total = np.array(results_total)\n",
        "  results_dict = defaultdict(list)\n",
        "\n",
        "  for row in results_total:\n",
        "    b, r, test_comparison_fraction, test_precision, test_recall,test_f1, test_f1_total = row\n",
        "    results_dict[(int(b), int(r))].append([test_comparison_fraction, test_precision, test_recall, test_f1,test_f1_total])\n",
        "\n",
        "  averaged_results = []\n",
        "  for (b, r), metrics in results_dict.items():\n",
        "    metrics_array = np.array(metrics)\n",
        "    avg_metrics = metrics_array.mean(axis=0)\n",
        "    averaged_results.append([b, r] + avg_metrics.tolist())\n",
        "\n",
        "  averaged_results = np.array(averaged_results)\n",
        "\n",
        "  return averaged_results"
      ],
      "metadata": {
        "id": "5iqyIsZ6USFc"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7smCxvif9vz"
      },
      "source": [
        "## Plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "ZNHgqnx3yaJU"
      },
      "outputs": [],
      "source": [
        "def plot_lsh_results(results, results_old):\n",
        "    comparison_fraction = results[:, 2]\n",
        "    pq = results[:, 3]\n",
        "    pc = results[:, 4]\n",
        "    f1star = results[:, 5]\n",
        "\n",
        "    comparison_fraction_old = results_old[:, 2]\n",
        "    pq_old = results_old[:, 3]\n",
        "    pc_old = results_old[:, 4]\n",
        "    f1star_old = results_old[:, 5]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(comparison_fraction, pq, marker='o', linestyle='-', color='b', label='MSMP+B')\n",
        "    plt.plot(comparison_fraction_old, pq_old, marker='x', linestyle='--', color='k', label='MSMP+')\n",
        "    plt.title('Pair quality vs. Fraction of Comparisons')\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('Pair Quality')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(comparison_fraction, pc, marker='o', linestyle='-', color='g', label='MSMP+B')\n",
        "    plt.plot(comparison_fraction_old, pc_old, marker='x', linestyle='--', color='k', label='MSMP+')\n",
        "    plt.title('Pair Completeness vs. Fraction of Comparisons')\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('Pair Completeness')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(comparison_fraction, f1star, marker='o', linestyle='-', color='r', label='MSMP+B')\n",
        "    plt.plot(comparison_fraction_old, f1star_old, marker='x', linestyle='--', color='k', label='MSMP+')\n",
        "    plt.title('F1* vs. Fraction of Comparisons')\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('F1*')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "BgIKzUNLG4Ma"
      },
      "outputs": [],
      "source": [
        "def plot_msm_results(results, results_old):\n",
        "    comparison_fraction = results[:, 2]\n",
        "    f1 = results[:, 5]\n",
        "\n",
        "    comparison_fraction_old = results_old[:, 2]\n",
        "    f1_old = results_old[:, 5]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(comparison_fraction, f1, marker='o', linestyle='-', color='r', label='MSMP+B')\n",
        "    plt.plot(comparison_fraction_old, f1_old, marker='x', linestyle='--', color='k', label='MSMP+')\n",
        "    plt.title('F1_MSM vs. Fraction of Comparisons')\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('F1_MSM')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_msm_results_total(results, results_old):\n",
        "    comparison_fraction = results[:, 2]\n",
        "    f1_total = results[:, 6]\n",
        "\n",
        "    comparison_fraction_old = results_old[:, 2]\n",
        "    f1_total_old = results_old[:, 6]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(comparison_fraction, f1_total, marker='o', linestyle='-', color='r', label='MSMP+B')\n",
        "    plt.plot(comparison_fraction_old, f1_total_old, marker='x', linestyle='--', color='k', label='MSMP+')\n",
        "    plt.title('F1_total vs. Fraction of Comparisons')\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('F1_total')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y8-KD16oZWEk"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBJBUrD1DZSp"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JZ66D6nS8p"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "cleaned_data = clean_features_title('TVs-all-merged.json', 'TVs-cleaned.json')\n",
        "with open(cleaned_data, 'r') as f:\n",
        "    clean_data = json.load(f)\n",
        "all_tvs = [item for model_id, items in clean_data.items() for item in items]\n",
        "results_LSH = bootstrapLSH(all_tvs,1200,5,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St-S6wmtDYHg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "cleaned_data_old = clean_features_title('TVs-all-merged.json', 'TVs-cleaned-old.json')\n",
        "with open(cleaned_data_old, 'r') as f:\n",
        "    clean_data_old = json.load(f)\n",
        "all_tvs_old = [item for model_id, items in clean_data_old.items() for item in items]\n",
        "results_LSH_old = bootstrapLSH(all_tvs_old,1200,5,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es5eewlIL0Xy"
      },
      "outputs": [],
      "source": [
        "plot_lsh_results(results_LSH, results_LSH_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X9JEQDbIqoL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results_MSM = bootstrapMSM(all_tvs,1200,5,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMZEGRA8Ioep"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results_MSM_old = bootstrapMSM(all_tvs_old,1200,5,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkOO3QSKe2x7"
      },
      "outputs": [],
      "source": [
        "plot_msm_results(results_MSM, results_MSM_old)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "results_MSM_total = bootstrapMSMtotal(all_tvs,1200,5,False)"
      ],
      "metadata": {
        "id": "vJvgL_A2aS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "results_MSM_old_total = bootstrapMSMtotal(all_tvs_old,1200,5,True)"
      ],
      "metadata": {
        "id": "k77bJq15aRkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_msm_results_total(results_MSM_total, results_MSM_old_total)"
      ],
      "metadata": {
        "id": "sm0Eb5poaaGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "q9NdSbqtbuDw",
        "fEo3sJ3pVBBR",
        "iaxX0-qfHsGY",
        "NVol4KAEH1me",
        "zgOLDOrWH-YP",
        "jjE4EEaFJFFZ",
        "UKJR_R-_H_9a",
        "aHOpOY18LGrF",
        "s7smCxvif9vz"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyN1NvkRMmiUyUGMrJPCqtcg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}